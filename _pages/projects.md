---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---

## Course Projects

- **A Brief Review of Sparse Principal Components Analysis and its Generalization**  [Report](https://github.com/ArkaB-DS/SPCA/blob/main/Multivariate_Project.pdf){: .btn--research} [Slides](https://github.com/ArkaB-DS/SPCA/blob/main/Multivariate_Project__slides_.pdf){: .btn--research} [Codes](https://github.com/ArkaB-DS/SPCA){: .btn--research}   
_Reviewed SPCA and General Adaptive SPCA as dimension reduction techniques and applied it to simulated and real data._
  <details>
      <summary>Abstract</summary>

  <blockquote>
    Principal Component Analysis is a widely studied methodology as it is a useful technique for dimension reduction. In this report, we discuss Sparse Principal Component Analysis (SPCA), which is a modification over PCA. This method is able to resolve the interpretation issue of PCA. Additionally, it provides sparse loadings to the principal components. The main idea of SPCA comes from the relationship between PCA problem and regression analysis. We also discuss GAS-PCA, which is a generalization over SPCA and this method performs better than SPCA, even in finite sample cases. Our report is mainly based on <a href="https://doi.org/10.1198/106186006X113430">Zou et al. (2006)</a> and its extension <a href="https://doi.org/10.1198/jcgs.2009.0012">Leng and Wang (2009)</a>.
  </blockquote>
  
  </details>
  
- **Nonparametric Kernel Density Estimation for the Metropolis-Hastings Algorithm**  [Report](https://github.com/ArkaB-DS/NDE4MH/blob/main/Nonparametric_Project.pdf){: .btn--research} [Slides](https://github.com/ArkaB-DS/NDE4MH/blob/main/Nonparametric_Project__slides_.pdf){: .btn--research} [Codes](https://github.com/ArkaB-DS/NDE4MH){: .btn--research}   
  <details>
      <summary>Abstract</summary>

  <blockquote>
    In this report, we discuss how the rejection step of the Metropolis-Hastings algorithm affects kernel density estimation. We elaborate on the theory developed by <a href="Sköld, M., & Roberts, G. O. (2003). Density estimation for the Metropolis–Hastings algorithm. Scandinavian journal of statistics, 30(4), 699-718.">Roberts et al. (2003)</a> by providing extensive proofs and explore applications exhibiting their efficiency in various problems.
  </blockquote>
  
  </details>
  
- **Spectral Clustering: Theory and Applications**  [Report](https://github.com/ArkaB-DS/SpectralClustering/blob/main/AI_Project.pdf){: .btn--research} [Codes]([https://github.com/ArkaB-DS/rankLASSO](https://github.com/ArkaB-DS/SpectralClustering)){: .btn--research}
  <details>
      <summary>Abstract</summary>

  <blockquote>
    In this report, we present a class of popular clustering algorithms called Spectral Clustering algorithms. We introduce graph theoretic notations required to understand the report. We discuss similarity graphs and graph Laplacians, along with their important properties. Three popular clustering algorithms are presented. Choice of optimal number of clusters, similarity functions, similarity graphs and graph Laplacians are also discussed. We then present Spectral clustering through different looking glasses. Finally, we apply Spectral clustering to simulated and real life datasets. This report is primarily based on <a href="https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s11222-007-9033-z.pdf&casa_token=D38DQJHbX-MAAAAA:rsNOf6rDvoZtSZPJVeLIVAKHoIsjui8ZR_qrJl8LhEuursk8T-IuBfM4Ov_TA3u9Tik5ewUhTbuKiX0">Von Luxburg (2007)</a>.
  </blockquote>
  
  </details>
  
- **Efficient High-dimensional Robust Variable Selection via Rank-based LASSO Methods**  [Report](https://github.com/ArkaB-DS/rankLASSO/blob/main/Robust_Project_II.pdf){: .btn--research} [Slides](https://github.com/ArkaB-DS/rankLASSO/blob/main/Robust_Project_II__slides_.pdf){: .btn--research} [Codes](https://github.com/ArkaB-DS/rankLASSO){: .btn--research}
  <details>
      <summary>Abstract</summary>

  <blockquote>
    Penalized variable selection is a popular approach for describing the relationship between the response,  and explanatory variables, . LASSO-based methods have received special attention throughout the literature of regression analysis. But stringent conditions are imposed on the  relation and on the error distribution. In this report, we present Rank-LASSO as a robust, superior method over the general LASSO, which can be used even when number of predictors is much larger than the sample size. The major properties of the Rank-LASSO has been presented in a non-asymptotic fashion, which makes it useful for the aforementioned case of . The report also shows the superiority of the thresholded modified version of Rank-LASSO in more general scenarios. Apart from theoretical results, we present numerical experiments for demonstrating that performance of the Rank-LASSO is substantially better than regular LAD-LASSO in terms of robust model selection problems. The report is primarily based on <a href="https://www.jmlr.org/papers/volume21/20-120/20-120.pdf">Rejchel, W., & Bogdan, M. (2020)</a>.
  </blockquote>
  
  </details>

- **Understanding Nonparametric Modal Regression via Kernel Density Estimation**  [Report](https://github.com/ArkaB-DS/NPmodalReg/blob/main/Group7%20Report.pdf){: .btn--research} [Slides](https://github.com/ArkaB-DS/NPmodalReg/blob/main/Robust_Project_I__slides_.pdf){: .btn--research} [Codes](https://github.com/ArkaB-DS/NPmodalReg)){: .btn--research}
  <details>
      <summary>Abstract</summary>

  <blockquote>
    In this report we review non-parametric Modal Regression using Kernel Density Estimator. Instead of using conditional mean, Modal Regression uses conditional mode to summarize the relationship between the response and the explanatory variables. We describe the idea of Modal Regression and include a brief discussion regarding the superiority of Multi-modal regression over the Uni-modal case. The consistency properties of the proposed estimator and the idea of Confidence Sets have been reviewed. This report also includes an application of Prediction Sets in case of Bandwidth selection. Certain generalizations and extensions are also discussed. The report is primarily based on <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Nonparametric-modal-regression/10.1214/15-AOS1373.pdf">Chen et al. (2016)</a>.
  </blockquote>
  
  </details>
    
- **Understanding Confidence Intervals in Adaptive Markov Chain Monte Carlo**  [Report](https://github.com/ArkaB-DS/MTH598A/blob/main/Report.pdf){: .btn--research} [Codes](https://github.com/ArkaB-DS/MTH598A){: .btn--research}
  <details>
      <summary>Abstract</summary>
  
  <blockquote>
  In this report, we attempt to understand the problems in asymptotic variance estimation for Adaptive Markov Chain Monte Carlo (AMCMC) and the role of confidence intervals in providing consistent estimation procedures for the asymptotic variance. The report is primarily based on <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.765.8899&rep=rep1&type=pdf">Atchade´ (2012)</a>.
  </blockquote>
  </details>
  
- **Ozone concentration and meteorology in the LA Basin, 1976 - A Regression Study**  [Report](https://github.com/ArkaB-DS/regressionProjectIITK/blob/main/Report/Project_Report.pdf){: .btn--research} [Slides](https://github.com/ArkaB-DS/regressionProjectIITK/blob/main/Presentation/Project_PPT.pdf){: .btn--research} [Codes](https://github.com/ArkaB-DS/regressionProjectIITK){: .btn--research}
  <details>
      <summary>Details</summary>
   
  <blockquote>
     - Performed Exploratory Data Analysis on the Ozone (LA Basin, 1976) dataset to understand the effect of meteorological variables in predicting Ozone concentration. 
     - Confirmed multicollinearity, heteroscedasticity, normality, and auto-correlation with appropriate tests and took corrective measures for each, developing three parametric predictive models.
     - Implemented Alternating Conditional Expectation (ACE) algorithm to create a non-parametric model that improved R^2 by 8% and RMSE by 62% with respect to the best of the three parametric models.
  </blockquote>
  </details>
